# neuralnetwork

Attempts to understand ANN's and implement some stuff thats not available in standard libraries(dropout, regularisation etc).

I've tried to make the code as modular as possible.

## Libraries:
pprint
numpy
sklearn

- and the dependencies for the above.

## Usage:
**python nn_class.py**

The command will train on MNIST digit dataset provided in scikit-learn with the following settings:
 - 3 hidden layers(you can add as many as you want)
 - batch size of 10
 - tanh nonlinearity
 - Cross Entropy cost
